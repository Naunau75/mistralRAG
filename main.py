import os
from typing import List
import glob
from langchain_community.document_loaders import PyPDFLoader
# Chargement des variables d'environnement
from dotenv import load_dotenv
load_dotenv()

# Pydantic pour la validation des donn√©es et de la config
from pydantic import BaseModel, Field, SecretStr

# LangChain pour l'orchestration
from langchain_mistralai import MistralAIEmbeddings, ChatMistralAI
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- 1. CONFIGURATION AVEC PYDANTIC ---
# L'int√©r√™t : Si tu oublies la cl√© API ou l'URI Mongo, le script s'arr√™te imm√©diatement avec une erreur claire.
class RagConfig(BaseModel):
    reset_db: bool = Field(False, description="Reset la base de donn√©es")
    mistral_api_key: str = Field(..., description="Cl√© API Mistral")
    mongodb_uri: str = Field(..., description="URI de connexion MongoDB Atlas")
    db_name: str = Field("missrag_db", description="Nom de la base de donn√©es")
    collection_name: str = Field("rag_collection", description="Nom de la collection")
    index_name: str = Field("vector_index", description="Nom de l'index Vector Search sur Atlas")
    model_name: str = Field("mistral-small-latest", description="Mod√®le pour le chat")
    embedding_model: str = Field("mistral-embed", description="Mod√®le pour les vecteurs")
    chunk_size: int = Field(500, description="Taille des morceaux de texte")
    chunk_overlap: int = Field(50, description="Chevauchement entre les morceaux")

try:
    config = RagConfig(
        mistral_api_key=os.getenv("MISTRAL_API_KEY"),
        mongodb_uri=os.getenv("MONGODB_ATLAS_Cluster_URI"), # Assure-toi que cette variable est dans ton .env
        reset_db=False 
    )
except Exception as e:
    print(f"‚ùå Erreur de configuration : {e}")
    print("üí° Astuce : V√©rifie que MISTRAL_API_KEY et MONGODB_ATLAS_Cluster_URI sont bien dans ton fichier .env")
    exit()

# --- 2. VECTOR STORAGE & EMBEDDINGS ---
from pymongo import MongoClient
from langchain_mongodb import MongoDBAtlasVectorSearch

# Connexion √† MongoDB
try:
    client = MongoClient(config.mongodb_uri)
    # Test de connexion
    client.admin.command('ping')
    print("‚úÖ Connexion √† MongoDB Atlas r√©ussie !")
except Exception as e:
    print(f"‚ùå Impossible de se connecter √† MongoDB : {e}")
    exit()

collection = client[config.db_name][config.collection_name]

# On instancie l'objet d'embedding Mistral
embeddings = MistralAIEmbeddings(
    api_key=SecretStr(config.mistral_api_key),
    model=config.embedding_model
)

# Gestion du RESET de la base
if config.reset_db:
    print(f"üóëÔ∏è Option reset_db activ√©e : Suppression de tous les documents dans '{config.collection_name}'...")
    collection.delete_many({})

# Initialisation du VectorStore
# NOTE : Tu DOIS avoir cr√©√© un index de recherche vectorielle sur Atlas pour que cela fonctionne !
# Nom de l'index : 'vector_index' (par d√©faut)
# Dimensions : 1024 (pour mistral-embed)
vectorstore = MongoDBAtlasVectorSearch(
    collection=collection,
    embedding=embeddings,
    index_name=config.index_name,
    relevance_score_fn="cosine",
)

# --- GESTION INCREMENTALE DES PDFS ---
print("üïµÔ∏è  V√©rification des documents existants...")

# On r√©cup√®re tous les fichiers PDF du dossier
pdf_folder = "./pdf"
pdf_files = glob.glob(os.path.join(pdf_folder, "*.pdf"))

if not pdf_files:
    print(f"‚ö†Ô∏è Aucun fichier PDF trouv√© dans {pdf_folder}")
else:
    # On regarde ce qu'il y a d√©j√† dans la base Mongo
    # On r√©cup√®re tous les chemins 'source' distincts dans les m√©tadonn√©es
    existing_sources = set(collection.distinct("source"))
    
    print(f"üìö {len(existing_sources)} fichier(s) d√©j√† index√©(s) dans la base.")

    # On identifie les nouveaux √† ajouter
    new_files = []
    for pdf_path in pdf_files:
        # On normalise le chemin ou on compare les noms de fichiers pour √™tre robuste
        is_present = False
        filename = os.path.basename(pdf_path)
        
        # V√©rification simple : est-ce que le chemin exact ou le nom de fichier est dans les sources ?
        for source in existing_sources:
            if source == pdf_path or source.endswith(filename):
                is_present = True
                break
        
        if not is_present:
            new_files.append(pdf_path)
        else:
            print(f"‚è© D√©j√† index√© : {pdf_path}")

    if not new_files:
        print("‚úÖ Tous les fichiers sont d√©j√† √† jour.")
    else:
        print(f"üöÄ {len(new_files)} nouveau(x) fichier(s) d√©tect√©(s). Traitement...")
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap
        )

        for pdf_path in new_files:
            print(f"üìÑ Traitement de : {pdf_path}")
            try:
                loader = PyPDFLoader(pdf_path)
                pages = loader.load()
                print(f"   ‚Ü≥ {len(pages)} pages charg√©es.")
                
                docs = text_splitter.split_documents(pages)
                print(f"   ‚Ü≥ {len(docs)} chunks g√©n√©r√©s. Indexation sur Atlas...")
                
                # Ajout incr√©mental √† la base (Chaque doc a 'source' dans metadata via PyPDFLoader)
                vectorstore.add_documents(docs)
                print("   ‚úÖ Ajout√© avec succ√®s.")
                
            except Exception as e:
                print(f"‚ùå Erreur lors du traitement de {pdf_path}: {e}")

# On transforme la base en "Retriever"
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})


# --- 4. LE PIPELINE RAG (LCEL - LangChain Expression Language) ---
print("üîó Construction du pipeline...")

# Le mod√®le de Chat
llm = ChatMistralAI(
    api_key=SecretStr(config.mistral_api_key),
    model=config.model_name,
    temperature=0
)

# Le Prompt Template
template = """R√©ponds √† la question uniquement bas√© sur le contexte suivant :
{context}

Question : {question}
"""
prompt = ChatPromptTemplate.from_template(template)

# D√©finition de la fonction pour formater les docs r√©cup√©r√©s (les coller ensemble)
def format_docs(docs):
    return "\n\n".join([d.page_content for d in docs])


rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# --- 5. EXECUTION ---

def ask(question: str):
    print(f"\n‚ùì Question : {question}")
    # invoke lance toute la cha√Æne d√©finie au-dessus
    response = rag_chain.invoke(question)
    print(f"ü§ñ R√©ponse : {response}")

# Tests
ask("Dis moi ce que tu connais sur le village de Soub√®s")
